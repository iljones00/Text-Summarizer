{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_path = config.model_path,\n",
    "            tokenizer_path = config.tokenizer_path,\n",
    "            metric_file_name = config.metric_file_name\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_metric, load_from_disk\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "    def calculate_metric_on_test_ds(self, dataset, metric, model, tokenizer,\n",
    "                                batch_size=16, \n",
    "                                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                column_text=\"dialogue\",\n",
    "                                column_summary=\"summary\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \\\n",
    "                padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), \\\n",
    "                attention_mask=inputs[\"attention_mask\"].to(device), \\\n",
    "                length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "            # Finally, we decode the generated texts,\n",
    "            # replace the  token, and add the decoded texts with the references to the metric.\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \\\n",
    "                                    clean_up_tokenization_spaces=True) for s in summaries]\n",
    "\n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "        #  Compute Rouge score.\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "    def evaluate(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "\n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "        score = self.calculate_metric_on_test_ds(dataset_samsum_pt[\"test\"], rouge_metric, model_pegasus, tokenizer)\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index=['pegasus'])\n",
    "        df.to_csv(self.config.metric_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-10 16:23:05: INFO: textSummarizerLogger: Read yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-11-10 16:23:05: INFO: textSummarizerLogger: Read yaml file: params.yaml loaded successfully]\n",
      "[2023-11-10 16:23:05: INFO: textSummarizerLogger: Created directory: artifacts]\n",
      "[2023-11-10 16:23:05: INFO: textSummarizerLogger: Created directory: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.70 GiB is allocated by PyTorch, and 52.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Isaia\\Desktop\\Text-Summarizer\\research\\05_model_evaluation.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_evaluation\u001b[39m.\u001b[39mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\Isaia\\Desktop\\Text-Summarizer\\research\\05_model_evaluation.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_evaluation_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_evaluation_config()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_evaluation \u001b[39m=\u001b[39m ModelEvaluation(model_evaluation_config)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_evaluation\u001b[39m.\u001b[39;49mevaluate()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32mc:\\Users\\Isaia\\Desktop\\Text-Summarizer\\research\\05_model_evaluation.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m rouge_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mrouge1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrouge2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrougeL\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrougeLsum\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m rouge_metric \u001b[39m=\u001b[39m load_metric(\u001b[39m\"\u001b[39m\u001b[39mrouge\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_metric_on_test_ds(dataset_samsum_pt[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m], rouge_metric, model_pegasus, tokenizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m rouge_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((rn, score[rn]\u001b[39m.\u001b[39mmid\u001b[39m.\u001b[39mfmeasure) \u001b[39mfor\u001b[39;00m rn \u001b[39min\u001b[39;00m rouge_names)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(rouge_dict, index\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpegasus\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\Isaia\\Desktop\\Text-Summarizer\\research\\05_model_evaluation.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m article_batch, target_batch \u001b[39min\u001b[39;00m tqdm(\u001b[39mzip\u001b[39m(article_batches, target_batches), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(article_batches)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(article_batch, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,  truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     summaries \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49minputs[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         length_penalty\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m, num_beams\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Finally, we decode the generated texts,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Isaia/Desktop/Text-Summarizer/research/05_model_evaluation.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# replace the  token, and add the decoded texts with the references to the metric.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\transformers\\generation\\utils.py:1496\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1488\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1489\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1490\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1491\u001b[0m         )\n\u001b[0;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1494\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1496\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0;32m   1497\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[0;32m   1498\u001b[0m     )\n\u001b[0;32m   1500\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\transformers\\generation\\utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    659\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 661\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[0;32m    663\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:820\u001b[0m, in \u001b[0;36mPegasusEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    813\u001b[0m         layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    814\u001b[0m             create_custom_forward(encoder_layer),\n\u001b[0;32m    815\u001b[0m             hidden_states,\n\u001b[0;32m    816\u001b[0m             attention_mask,\n\u001b[0;32m    817\u001b[0m             (head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[0;32m    821\u001b[0m             hidden_states,\n\u001b[0;32m    822\u001b[0m             attention_mask,\n\u001b[0;32m    823\u001b[0m             layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    824\u001b[0m             output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    827\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    829\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:336\u001b[0m, in \u001b[0;36mPegasusEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    334\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    335\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m--> 336\u001b[0m hidden_states, attn_weights, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    337\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    338\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    339\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    340\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    341\u001b[0m )\n\u001b[0;32m    342\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    343\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Isaia\\Anaconda3\\envs\\text_sum\\lib\\site-packages\\transformers\\models\\pegasus\\modeling_pegasus.py:239\u001b[0m, in \u001b[0;36mPegasusAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    236\u001b[0m value_states \u001b[39m=\u001b[39m value_states\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mproj_shape)\n\u001b[0;32m    238\u001b[0m src_len \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 239\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m attn_weights\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[0;32m    242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    243\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention weights should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39msrc_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    244\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_weights\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.70 GiB is allocated by PyTorch, and 52.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(model_evaluation_config)\n",
    "    model_evaluation.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
